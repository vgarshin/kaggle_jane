{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "print('tensorflow version:', tf.__version__)\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for gpu_device in gpu_devices:\n",
    "        print('device available:', gpu_device)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "VER = 'v1'\n",
    "PARAMS = {\n",
    "    'version': VER,\n",
    "    'folds': 5,\n",
    "    'finetune': False,\n",
    "    'epochs': 1000,\n",
    "    'min_epochs': 5,\n",
    "    'batch_size': 4096,\n",
    "    'validation_split': .1,\n",
    "    'patience': 20,\n",
    "    'group_gap': 20,\n",
    "    'max_trials': 50,\n",
    "    'seed': 2020,\n",
    "    'lr': .001,\n",
    "    'min_lr': 1e-5,\n",
    "    'noise': .1,\n",
    "    'comments': ''\n",
    "}\n",
    "DATA_PATH = './data'\n",
    "MDLS_PATH = f'./models_{VER}'\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "TH = .5\n",
    "    \n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_all(PARAMS['seed'])\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'{DATA_PATH}/train.csv')\n",
    "train = train.query('date > 85').reset_index(drop=True) \n",
    "train = train.astype({c: np.float32 for c in \n",
    "                      train.select_dtypes(include='float64').columns})\n",
    "train.fillna(train.mean(), inplace=True)\n",
    "train = train.query('weight > 0').reset_index(drop=True)\n",
    "train['action'] = (\n",
    "    (train['resp_1'] > 0) & \n",
    "    (train['resp_2'] > 0) & \n",
    "    (train['resp_3'] > 0) & \n",
    "    (train['resp_4'] > 0) & \n",
    "    (train['resp'] > 0)\n",
    ").astype('int')\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "X = train[features].values\n",
    "y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n",
    "f_mean = np.mean(train[features[1:]].values, axis=0)\n",
    "PARAMS['features'] = features\n",
    "PARAMS['X_size'] = X.shape[-1]\n",
    "PARAMS['y_size'] = y.shape[-1]\n",
    "with open(f'{MDLS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(PARAMS, file)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"\n",
    "    Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, n_splits=5, *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None, verbose=False):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError('The \"groups\" parameter should not be None')\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                'Cannot have number of' + \n",
    "                'folds={0} greater than the number of groups={1}'.format(\n",
    "                    n_folds,\n",
    "                    n_groups\n",
    "                )\n",
    "            )\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits*group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(\n",
    "                    np.unique(\n",
    "                        np.concatenate((train_array, train_array_tmp)),\n",
    "                        axis=None\n",
    "                    ), axis=None\n",
    "                )\n",
    "            train_end = train_array.size\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(\n",
    "                    np.unique(\n",
    "                        np.concatenate((test_array, test_array_tmp)),\n",
    "                        axis=None), \n",
    "                    axis=None\n",
    "                )\n",
    "            test_array  = test_array[group_gap:]\n",
    "            if self.verbose > 0:\n",
    "                pass\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    \n",
    "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1, callbacks=None):\n",
    "        val_losses = []\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
    "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
    "            if len(X_train) < 2:\n",
    "                X_train = X_train[0]\n",
    "                X_test = X_test[0]\n",
    "            if len(y_train) < 2:\n",
    "                y_train = y_train[0]\n",
    "                y_test = y_test[0]\n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(\n",
    "                X_train,y_train,\n",
    "                validation_data=(X_test,y_test),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(\n",
    "            trial.trial_id, \n",
    "            {k:np.mean(val_losses[:, i]) for i,k in enumerate(hist.history.keys())}\n",
    "        )\n",
    "        self.save_model(trial.trial_id, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim, output_dim, lr=.001, noise=.05):\n",
    "    inputs = Input(input_dim)\n",
    "    encoded = BatchNormalization()(inputs)\n",
    "    encoded = GaussianNoise(noise)(encoded)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dropout(.2)(encoded)\n",
    "    decoded = Dense(input_dim, name='decoded')(decoded)\n",
    "    x = Dense(32, activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    x = Dense(output_dim, activation='sigmoid', name='label_output')(x)\n",
    "    encoder = Model(inputs=inputs, outputs=encoded)\n",
    "    autoencoder = Model(inputs=inputs, outputs=[decoded, x])\n",
    "    autoencoder.compile(\n",
    "        optimizer=Adam(lr),\n",
    "        loss={'decoded': 'mse', 'label_output': 'binary_crossentropy'}\n",
    "    )\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp, input_dim, output_dim, encoder, lr=.001):\n",
    "    inputs = Input(input_dim)\n",
    "    x = encoder(inputs)\n",
    "    x = Concatenate()([x, inputs]) #use both raw and encoded features\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('init_dropout', 0, .5))(x)\n",
    "    for i in range(hp.Int('num_layers', 1, 4)):\n",
    "        x = Dense(hp.Int(f'num_units_{i}', 64, 512))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}', .0, .5))(x)\n",
    "    x = Dense(output_dim, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Float('lr', .00001, .1, default=lr)),\n",
    "        loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing', .0, .1)),\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_autoencoder(PARAMS['X_size'], PARAMS['y_size'], \n",
    "                                          lr=PARAMS['lr'], noise=PARAMS['noise'])\n",
    "if TRAINING:\n",
    "    callbacks=[\n",
    "        EarlyStopping('val_loss', patience=PARAMS['patience'], \n",
    "                      restore_best_weights=True),\n",
    "        ReduceLROnPlateau('val_loss', patience=int(PARAMS['patience'] / 2),\n",
    "                          factor=.1, verbose=0, min_lr=PARAMS['min_lr'])\n",
    "    ]\n",
    "    autoencoder.fit(\n",
    "        X, \n",
    "        (X, y),\n",
    "        epochs=PARAMS['epochs'],\n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        validation_split=PARAMS['validation_split'],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    encoder.save_weights(f'{MDLS_PATH}/encoder.hdf5')\n",
    "else:\n",
    "    encoder.load_weights(f'{MDLS_PATH}/encoder.hdf5')\n",
    "    print('encoder weights loaded')\n",
    "encoder.trainable = False\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = lambda hp: create_model(hp, PARAMS['X_size'], PARAMS['y_size'], \n",
    "                                   encoder, lr=PARAMS['lr'])\n",
    "if TRAINING:\n",
    "    tuner = CVTuner(\n",
    "        hypermodel=model_fn,\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "            objective= kt.Objective('val_auc', direction='max'),\n",
    "            num_initial_points=4,\n",
    "            max_trials=PARAMS['max_trials']\n",
    "        )\n",
    "    )\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits=PARAMS['folds'], \n",
    "                                     group_gap=PARAMS['group_gap'])\n",
    "    splits = list(gkf.split(y, groups=train['date'].values))\n",
    "    print('=' * 10, f'TUNER max trials={PARAMS[\"max_trials\"]}', '=' * 10)\n",
    "    tuner.search(\n",
    "        (X, ), (y, ),\n",
    "        splits=splits,\n",
    "        batch_size=PARAMS['batch_size'],\n",
    "        epochs=PARAMS['epochs'],\n",
    "        callbacks=[EarlyStopping('val_auc', mode='max', \n",
    "                                 patience=PARAMS['patience'])]\n",
    "    )\n",
    "    hp = tuner.get_best_hyperparameters(1)[0]\n",
    "    pd.to_pickle(hp, f'{MDLS_PATH}/best_hp.pkl', protocol=4)\n",
    "    for fold, (train_indices, test_indices) in enumerate(splits):\n",
    "        print('=' * 10, f'FOLD {fold}', '=' * 10)\n",
    "        model = model_fn(hp)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test), \n",
    "            epochs=PARAMS['epochs'],\n",
    "            batch_size=PARAMS['batch_size'],\n",
    "            callbacks=[EarlyStopping('val_auc', mode='max', \n",
    "                                     patience=PARAMS['patience'],\n",
    "                                     restore_best_weights=True)]\n",
    "        )\n",
    "        model.save_weights(f'{MDLS_PATH}/model_f{fold}.hdf5')\n",
    "        model.compile(Adam(hp.get('lr') / 100), loss='binary_crossentropy')\n",
    "        model.fit(\n",
    "            X_test, y_test, \n",
    "            epochs=PARAMS['min_epochs'],\n",
    "            batch_size=PARAMS['batch_size']\n",
    "        )\n",
    "        model.save_weights(f'{MDLS_PATH}/model_f{fold}_finetune.hdf5')\n",
    "    tuner.results_summary()\n",
    "else:\n",
    "    models = []\n",
    "    hp = pd.read_pickle(f'{MDLS_PATH}/best_hp.pkl')\n",
    "    for fold in range(PARAMS['folds']):\n",
    "        model = model_fn(hp)\n",
    "        if PARAMS['finetune']:\n",
    "            model.load_weights(f'{MDLS_PATH}/model_f{fold}_finetune.hdf5')\n",
    "        else:\n",
    "            model.load_weights(f'{MDLS_PATH}/model_f{fold}.hdf5')\n",
    "        models.append(model)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    models[0].summary()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    import janestreet\n",
    "    ENV = janestreet.make_env()\n",
    "    f = np.median\n",
    "    models = models[-2:]\n",
    "    for (test_df, pred_df) in tqdm(ENV.iter_test()):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_tt = test_df.loc[:, features].values\n",
    "            if np.isnan(x_tt[:, 1:].sum()):\n",
    "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "            pred = np.mean([model(x_tt, training = False).numpy() for model in models], axis=0)\n",
    "            pred = f(pred)\n",
    "            pred_df.action = np.where(pred >= TH, 1, 0).astype(int)\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "        ENV.predict(pred_df)\n",
    "else:\n",
    "    print('no submission env created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
